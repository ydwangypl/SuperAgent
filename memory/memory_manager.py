#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è®°å¿†ç®¡ç†å™¨

å®žçŽ°3å±‚è®°å¿†ç³»ç»Ÿ,é˜²æ­¢é‡å¤é”™è¯¯,åŠ é€Ÿé¡¹ç›®å¼€å‘
"""

import asyncio
import threading  # v3.3 ä¼˜åŒ–ï¼šä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤å•ä¾‹
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
import json
import uuid
import aiofiles
from common.monitoring import MetricsManager
from common.exceptions import MemorySystemError
from config.constants import Defaults  # P2: ä½¿ç”¨å¸¸é‡æ›¿ä»£é­”æ³•æ•°å­—

logger = logging.getLogger(__name__)

# å¸¸é‡å®šä¹‰
SECTION_MISTAKES = "## ðŸ“ é”™è¯¯ä¸Žæ•™è®­"
SECTION_PRACTICES = "## ðŸŽ¯ æœ€ä½³å®žè·µ"
SECTION_ARCHITECTURE = "## ðŸ—ï¸ æž¶æž„å†³ç­–"
SECTION_STATISTICS = "## ðŸ“Š é¡¹ç›®ç»Ÿè®¡"


@dataclass
class MemoryEntry:
    """è®°å¿†æ¡ç›®"""
    memory_id: str
    memory_type: str  # episodic, semantic, procedural
    timestamp: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    tags: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "memory_id": self.memory_id,
            "memory_type": self.memory_type,
            "timestamp": self.timestamp,
            "content": self.content,
            "metadata": self.metadata,
            "tags": self.tags
        }


class MemoryManager:
    """è®°å¿†ç®¡ç†å™¨ - å®žçŽ°3å±‚è®°å¿†ç³»ç»Ÿ (å•ä¾‹å¼‚æ­¥ç‰ˆ v3.4 ä¼˜åŒ–)"""

    _instance: Optional['MemoryManager'] = None
    _class_lock = threading.Lock()  # çº¿ç¨‹é”ä¿æŠ¤å•ä¾‹åˆ›å»º

    def __new__(cls, *args, **kwargs) -> 'MemoryManager':
        # ä½¿ç”¨é”ä¿æŠ¤å•ä¾‹åˆ›å»º
        with cls._class_lock:
            if not cls._instance:
                cls._instance = super(MemoryManager, cls).__new__(cls)
            return cls._instance

    def __init__(self, project_root: Optional[Path] = None) -> None:
        """åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨

        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•
        """
        # é˜²æ­¢é‡å¤åˆå§‹åŒ–
        if getattr(self, '_initialized', False):
            # å¦‚æžœå·²ç»åˆå§‹åŒ–ä½† project_root ä¸åŒï¼Œåªæ›´æ–°è·¯å¾„
            if project_root is not None and self.project_root != Path(project_root):
                self.project_root = Path(project_root)
            return

        self.project_root = project_root or Path.cwd()

        # é…ç½®å„å±‚è®°å¿†çš„å­˜å‚¨è·¯å¾„
        self.memory_dir = self.project_root / ".superagent" / "memory"
        self.episodic_dir = self.memory_dir / "episodic"
        self.semantic_dir = self.memory_dir / "semantic"
        self.procedural_dir = self.memory_dir / "procedural"

        # æŒç»­è®°å¿†æ–‡ä»¶
        self.continuity_file = self.memory_dir / "CONTINUITY.md"

        # ç´¢å¼•æ–‡ä»¶
        self.index_file = self.memory_dir / "memory_index.json"

        # é”
        self._lock = asyncio.Lock()  # å†…å­˜çŠ¶æ€é”
        self._io_lock = asyncio.Lock()  # æ–‡ä»¶å†™å…¥é” (é˜²æ­¢å¹¶å‘å†™å…¥åŒä¸€æ–‡ä»¶)
        self._cache_lock = threading.Lock()  # ç¼“å­˜æ“ä½œé”ï¼ˆä½¿ç”¨çº¿ç¨‹é”ï¼Œå› ä¸ºæ–¹æ³•æ˜¯åŒæ­¥çš„ï¼‰

        # åˆ›å»ºç›®å½•ç»“æž„
        self._init_directories()

        # åŠ è½½ç´¢å¼•
        self.index: Dict[str, Any] = self._load_index_sync()

        # åˆå§‹åŒ–æŸ¥è¯¢ç¼“å­˜ (ä½¿ç”¨ LRU + LFU æ··åˆç­–ç•¥)
        # ç¼“å­˜ç»“æž„: {type: {memory_id: (entry_dict, access_count, last_access_time)}}
        self._cache: Dict[str, Dict[str, tuple[Dict[str, Any], int, float]]] = {
            "episodic": {},
            "semantic": {},
            "procedural": {}
        }
        # ç±»åˆ«ç´¢å¼• (type -> category -> list of memory_ids)
        self._category_index: Dict[str, Dict[str, List[str]]] = {
            "semantic": {},
            "procedural": {}
        }
        self._cache_ttl = Defaults.CACHE_TTL.value  # ä½¿ç”¨å¸¸é‡
        self._max_cache_size = Defaults.MAX_CACHE_SIZE.value  # ä½¿ç”¨å¸¸é‡
        self._continuity_cache: Optional[str] = None  # CONTINUITY.md å†…å®¹ç¼“å­˜
        self._last_flush_time: float = 0.0

        # æ ‡è®°åˆå§‹åŒ–å®Œæˆ
        self._initialized = True
        self._index_building = False
        self._index_task = None
        self._index_ready = asyncio.Event()  # P0 Fix: åˆå§‹åŒ–ç´¢å¼•å°±ç»ªäº‹ä»¶
        self._index_ready.set()  # åˆå§‹çŠ¶æ€ä¸‹ç´¢å¼•å·²å®Œæˆ

        logger.info(f"è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ: {self.memory_dir}")

    @classmethod
    def get_instance(cls, project_root: Optional[Path] = None) -> 'MemoryManager':
        """èŽ·å–å•ä¾‹ï¼Œæ”¯æŒå»¶è¿Ÿåˆå§‹åŒ–

        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•ï¼ˆä»…åœ¨é¦–æ¬¡åˆ›å»ºæ—¶æœ‰æ•ˆï¼‰

        Returns:
            MemoryManager å•ä¾‹
        """
        if not cls._instance:
            with cls._class_lock:
                if not cls._instance:
                    instance = cls(project_root)
                    cls._instance = instance
        return cls._instance

    async def initialize_async(self, project_root: Optional[Path] = None) -> None:
        """å¼‚æ­¥åˆå§‹åŒ–ï¼ˆå¯é€‰ï¼Œç”¨äºŽéœ€è¦å¼‚æ­¥åˆå§‹åŒ–çš„åœºæ™¯ï¼‰

        P0 Fix: ä½¿ç”¨å¼‚æ­¥é”é¿å…çº¿ç¨‹/å¼‚æ­¥é”æ··ç”¨å¯¼è‡´çš„æ­»é”
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•
        """
        async with self._lock:
            if not self._index_building:
                self._index_building = True
                self._index_task = asyncio.create_task(self._build_category_index())

    async def wait_for_index(self, timeout: float = 30.0) -> bool:
        """ç­‰å¾…ç±»åˆ«ç´¢å¼•æž„å»ºå®Œæˆ

        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            æ˜¯å¦åœ¨è¶…æ—¶å‰å®Œæˆ
        """
        if not self._index_building:
            return True  # å·²ç»å®Œæˆ

        if self._index_task is None:
            return True

        try:
            await asyncio.wait_for(self._index_task, timeout=timeout)
            return True
        except asyncio.TimeoutError:
            logger.warning("ç­‰å¾…ç±»åˆ«ç´¢å¼•æž„å»ºè¶…æ—¶")
            return False

    async def _build_category_index(self) -> None:
        """å¼‚æ­¥æž„å»ºç±»åˆ«ç´¢å¼• (v3.3 ä¼˜åŒ–ç‰ˆï¼šIOä¸åœ¨é”å†… + å®Œæˆé€šçŸ¥)"""
        try:
            for mtype in ["semantic", "procedural"]:
                # 1. å…ˆåœ¨ä¸å é”çš„æƒ…å†µä¸‹æ”¶é›†æ‰€æœ‰ ID
                async with self._lock:
                    mids = list(self.index.get(mtype, []))

                for mid in mids:
                    try:
                        folder = (self.semantic_dir if mtype == "semantic"
                                  else self.procedural_dir)
                        file_path = folder / f"{mid}.json"

                        if file_path.exists():
                            # 2. è¯»å–æ–‡ä»¶ (IO)
                            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                                content = await f.read()
                                memory_data = json.loads(content)
                                category = memory_data.get("metadata", {}).get(
                                    "category", "general"
                                )

                                # 3. æ›´æ–°å†…å­˜ä¸­çš„ç´¢å¼• (å é”)
                                async with self._lock:
                                    if category not in self._category_index[mtype]:
                                        self._category_index[mtype][category] = []
                                    if mid not in self._category_index[mtype][category]:
                                        self._category_index[mtype][category].append(mid)
                    except (FileNotFoundError, json.JSONDecodeError, PermissionError) as e:
                        logger.error(f"æž„å»ºç±»åˆ«ç´¢å¼•å¤±è´¥ - æ–‡ä»¶æŸåæˆ–ä¸¢å¤± ({mid}): {e}")
                    except (OSError, IOError) as e:
                        logger.error(f"æž„å»ºç±»åˆ«ç´¢å¼•å¤±è´¥ - IOé”™è¯¯ ({mid}): {e}")
                    except Exception as e:
                        logger.error(f"æž„å»ºç±»åˆ«ç´¢å¼•å¤±è´¥ - æœªçŸ¥é”™è¯¯ ({type(e).__name__}) ({mid}): {e}")
        finally:
            # v3.3: é€šçŸ¥ç´¢å¼•æž„å»ºå®Œæˆ
            self._index_building = False
            self._index_ready.set()
            logger.info("ç±»åˆ«ç´¢å¼•æž„å»ºå®Œæˆ")

    def _init_directories(self) -> None:
        """åˆå§‹åŒ–ç›®å½•ç»“æž„"""
        for d in [self.memory_dir, self.episodic_dir, self.semantic_dir, self.procedural_dir]:
            d.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–CONTINUITY.md(å¦‚æžœä¸å­˜åœ¨)
        if not self.continuity_file.exists():
            self._init_continuity_file_sync()

    def _generate_id(self, prefix: str) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„è®°å¿†ID"""
        return f"{prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"

    def _init_continuity_file_sync(self) -> None:
        content = f"""# SuperAgent v3.2 - æŒç»­è®°å¿† (CONTINUITY)

> æ­¤æ–‡ä»¶ç”±SuperAgentè‡ªåŠ¨ç»´æŠ¤,è®°å½•é¡¹ç›®å¼€å‘è¿‡ç¨‹ä¸­çš„é‡è¦ç»éªŒã€é”™è¯¯æ•™è®­å’Œæœ€ä½³å®žè·µ

---

## ðŸ“ é”™è¯¯ä¸Žæ•™è®­ (Mistakes & Learnings)

---

## ðŸŽ¯ æœ€ä½³å®žè·µ (Best Practices)

---

## ðŸ—ï¸ æž¶æž„å†³ç­– (Architecture Decisions)

---

## ðŸ“Š é¡¹ç›®ç»Ÿè®¡ (Project Statistics)

- **æ€»è®°å¿†æ¡ç›®**: 0
- **æƒ…èŠ‚è®°å¿†**: 0
- **è¯­ä¹‰è®°å¿†**: 0
- **ç¨‹åºè®°å¿†**: 0
- **æœ€åŽæ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        self.continuity_file.write_text(content, encoding='utf-8')

    def _load_index_sync(self) -> Dict[str, Any]:
        """åŒæ­¥åŠ è½½è®°å¿†ç´¢å¼•, å¢žåŠ å®¹é”™é€»è¾‘"""
        default_index = {
            "episodic": [],
            "semantic": [],
            "procedural": [],
            "total_count": 0
        }

        if not self.index_file.exists():
            return default_index

        try:
            with open(self.index_file, 'r', encoding='utf-8') as f:
                content = f.read()
                if not content.strip():
                    return default_index
                return json.loads(content)
        except (json.JSONDecodeError, IOError) as e:
            logger.error(f"åŠ è½½è®°å¿†ç´¢å¼•å¤±è´¥, æ–‡ä»¶å¯èƒ½å·²æŸå: {e}ã€‚æ­£åœ¨åˆ›å»ºæ–°ç´¢å¼•ã€‚")
            # å¤‡ä»½æŸåçš„æ–‡ä»¶
            try:
                import shutil
                backup_path = self.index_file.with_suffix('.bak')
                shutil.copy2(self.index_file, backup_path)
                logger.info(f"å·²å°†æŸåçš„ç´¢å¼•å¤‡ä»½è‡³: {backup_path}")
            except (shutil.Error, OSError) as e:
                logger.warning(f"å¤‡ä»½æŸåçš„ç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")
            return default_index

    def _get_from_cache(self, memory_type: str, memory_id: str) -> Optional[Dict[str, Any]]:
        """ä»Žç¼“å­˜èŽ·å–è®°å¿†æ¡ç›® (v3.3 LRU + LFU æ··åˆç­–ç•¥)

        Args:
            memory_type: è®°å¿†ç±»åž‹
            memory_id: è®°å¿† ID

        Returns:
            ç¼“å­˜çš„æ¡ç›®æˆ– None
        """
        if memory_type not in self._cache:
            return None

        # v3.3 ä¼˜åŒ–ï¼šä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤ï¼Œç¡®ä¿åŽŸå­æ€§
        with self._cache_lock:
            cached_entry_data = self._cache[memory_type].get(memory_id)
            if not cached_entry_data:
                return None

            entry, access_count, timestamp = cached_entry_data
            if time.time() - timestamp > self._cache_ttl:
                del self._cache[memory_type][memory_id]
                return None

            # è®¿é—®æ—¶æ›´æ–° LFU è®¡æ•°å’Œ LRU æ—¶é—´æˆ³ (åŽŸå­æ“ä½œ)
            self._cache[memory_type][memory_id] = (entry, access_count + 1, time.time())
            return entry

    def _save_to_cache(self, memory_type: str, memory_id: str, entry: Dict[str, Any]) -> None:
        """ä¿å­˜è®°å¿†æ¡ç›®åˆ°ç¼“å­˜ (v3.3 LRU + LFU æ··åˆæ·˜æ±°ç­–ç•¥)"""
        if memory_type not in self._cache:
            return

        # v3.3 ä¼˜åŒ–ï¼šä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤
        with self._cache_lock:
            # å½“ç¼“å­˜æ»¡æ—¶ï¼Œä½¿ç”¨ LFU + LRU æ··åˆç­–ç•¥æ·˜æ±°
            # ä¼˜å…ˆæ·˜æ±°è®¿é—®é¢‘çŽ‡ä½Žçš„æ¡ç›®ï¼Œé¢‘çŽ‡ç›¸åŒæ—¶æ·˜æ±°æœ€ä¹…æœªè®¿é—®çš„
            if len(self._cache[memory_type]) >= self._max_cache_size:
                candidates = []
                for mid, (_entry, count, ts) in self._cache[memory_type].items():
                    candidates.append((mid, count, ts))

                # æ‰¾å‡ºè®¿é—®é¢‘çŽ‡æœ€ä½Žçš„æ¡ç›®ï¼ˆä¿ç•™è‡³å°‘è®¿é—®è¿‡ä¸€æ¬¡çš„ï¼‰
                min_count = min(c[1] for c in candidates) if candidates else 0
                low_freq_items = [c for c in candidates if c[1] == min_count]

                # ä»Žä½Žé¢‘æ¡ç›®ä¸­æ‰¾å‡ºæœ€ä¹…æœªè®¿é—®çš„
                oldest = min(low_freq_items, key=lambda c: c[2])
                del self._cache[memory_type][oldest[0]]
                logger.debug(f"ç¼“å­˜æ·˜æ±°: {oldest[0]} (è®¿é—®æ¬¡æ•°: {oldest[1]})")

            # ä¿å­˜æ–°æ¡ç›® (access_count=0 è¡¨ç¤ºåˆšæ·»åŠ )
            self._cache[memory_type][memory_id] = (entry, 0, time.time())

    def _clean_expired_cache(self) -> None:
        """æ¸…ç†è¿‡æœŸç¼“å­˜ (v3.3 ä¼˜åŒ–)"""
        now = time.time()
        cleaned = 0
        for memory_type in self._cache:
            with self._cache_lock:
                expired_keys = [
                    k for k, (_entry, _count, t) in self._cache[memory_type].items()
                    if now - t > self._cache_ttl
                ]
                for k in expired_keys:
                    del self._cache[memory_type][k]
                    cleaned += 1

        if cleaned > 0:
            logger.debug(f"æ¸…ç†è¿‡æœŸç¼“å­˜: {cleaned} ä¸ªæ¡ç›®")

    async def _save_index(self) -> None:
        """å¼‚æ­¥ä¿å­˜è®°å¿†ç´¢å¼• (å¸¦ IO é”ä¿æŠ¤)"""
        try:
            # 1. å…ˆå‡†å¤‡è¦å†™å…¥çš„å†…å®¹ (å†…å­˜æ“ä½œ)
            async with self._lock:
                content = json.dumps(self.index, indent=2, ensure_ascii=False)

            # 2. èŽ·å– IO é”å¹¶å†™å…¥æ–‡ä»¶
            async with self._io_lock:
                # ä½¿ç”¨åŽŸå­å†™å…¥ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶å†é‡å‘½å
                temp_file = self.index_file.with_suffix('.tmp')
                async with aiofiles.open(temp_file, 'w', encoding='utf-8') as f:
                    await f.write(content)

                # Windows ä¸‹ replace å‰å¦‚æžœç›®æ ‡æ–‡ä»¶å­˜åœ¨éœ€è¦å¤„ç†ï¼Œä½† Path.replace åº”è¯¥å¯ä»¥å¤„ç†
                temp_file.replace(self.index_file)

        except (OSError, IOError) as e:
            logger.error(f"ä¿å­˜è®°å¿†ç´¢å¼•å¤±è´¥ (IOé”™è¯¯): {e}")
            raise MemorySystemError(f"ä¿å­˜è®°å¿†ç´¢å¼•å¤±è´¥ (IOé”™è¯¯): {str(e)}")
        except (TypeError, ValueError) as e:
            logger.error(f"ä¿å­˜è®°å¿†ç´¢å¼•å¤±è´¥ (åºåˆ—åŒ–é”™è¯¯): {e}")
            raise MemorySystemError(f"ä¿å­˜è®°å¿†ç´¢å¼•å¤±è´¥ (åºåˆ—åŒ–é”™è¯¯): {str(e)}")
        except Exception as e:
            logger.error(f"ä¿å­˜è®°å¿†ç´¢å¼•é‡åˆ°æœªçŸ¥é”™è¯¯ ({type(e).__name__}): {e}")
            raise MemorySystemError(
                f"ä¿å­˜è®°å¿†ç´¢å¼•é‡åˆ°æœªçŸ¥é”™è¯¯ ({type(e).__name__}): {str(e)}"
            )

    async def _save_entry(self, entry: MemoryEntry, directory: Path) -> None:
        """é€šç”¨æ¡ç›®ä¿å­˜æ–¹æ³• (å·²ä¼˜åŒ–ï¼šå‰¥ç¦» IO é”)"""
        try:
            file_path = directory / f"{entry.memory_id}.json"
            entry_dict = entry.to_dict()

            # 1. å…ˆå†™æ¡ç›®æ–‡ä»¶ (ä¸å é”)
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(entry_dict, indent=2, ensure_ascii=False))

            # 2. æ›´æ–°ç¼“å­˜å’Œå†…å­˜ç´¢å¼• (å é”)
            async with self._lock:
                # ä¿å­˜åˆ°ç¼“å­˜
                self._save_to_cache(entry.memory_type, entry.memory_id, entry_dict)

                # æ›´æ–°ç´¢å¼•åˆ—è¡¨
                if entry.memory_id not in self.index[entry.memory_type]:
                    self.index[entry.memory_type].append(entry.memory_id)
                    self.index["total_count"] += 1

                # æ›´æ–°ç±»åˆ«ç´¢å¼•
                if entry.memory_type in self._category_index:
                    category = entry.metadata.get("category", "general")
                    if category not in self._category_index[entry.memory_type]:
                        self._category_index[entry.memory_type][category] = []
                    if entry.memory_id not in \
                            self._category_index[entry.memory_type][category]:
                        self._category_index[entry.memory_type][category].append(
                            entry.memory_id
                        )

            # 3. å¼‚æ­¥ä¿å­˜ç´¢å¼•æ–‡ä»¶ (IO å¯†é›†ï¼Œç§»å‡ºä¸»é”)
            await self._save_index()

            # 4. æ›´æ–°ç›‘æŽ§æŒ‡æ ‡
            MetricsManager.record_memory_op(entry.memory_type, "save", "success")
            # æ­¤æ—¶è®¿é—® self.index éœ€è¦æ³¨æ„å¹¶å‘ï¼Œä½†åœ¨ record è¿™ç§éžå…³é”®æ“ä½œä¸­é€šå¸¸ OK
            MetricsManager.update_memory_size(
                entry.memory_type,
                len(self.index[entry.memory_type])
            )

        except (OSError, IOError, PermissionError) as e:
            logger.error(f"ä¿å­˜è®°å¿†æ¡ç›®å¤±è´¥ (æ–‡ä»¶æˆ–ç£ç›˜é”™è¯¯): {e}")
            MetricsManager.record_memory_op(entry.memory_type, "save", "error")
            raise MemorySystemError(f"ä¿å­˜è®°å¿†æ¡ç›®å¤±è´¥ (IOé”™è¯¯): {str(e)}")
        except MemorySystemError:
            MetricsManager.record_memory_op(entry.memory_type, "save", "error")
            raise
        except Exception as e:
            logger.error(f"ä¿å­˜è®°å¿†æ¡ç›®å¤±è´¥ (æœªçŸ¥é”™è¯¯ - {type(e).__name__}): {e}")
            MetricsManager.record_memory_op(entry.memory_type, "save", "error")
            raise MemorySystemError(
                f"ä¿å­˜è®°å¿†æ¡ç›®é‡åˆ°æœªçŸ¥é”™è¯¯ ({type(e).__name__}): {str(e)}"
            )

    # ========== Episodic Memory (æƒ…èŠ‚è®°å¿†) ==========

    async def save_episodic_memory(
        self,
        event: str,
        task_id: Optional[str] = None,
        agent_type: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """ä¿å­˜æƒ…èŠ‚è®°å¿† - è®°å½•ä»»åŠ¡æ‰§è¡ŒåŽ†å²"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        memory_id = self._generate_id("episodic")

        entry = MemoryEntry(
            memory_id=memory_id,
            memory_type="episodic",
            timestamp=timestamp,
            content=event,
            metadata=metadata or {},
            tags=["episodic"]
        )

        if task_id:
            entry.tags.append(f"task:{task_id}")
        if agent_type:
            entry.tags.append(f"agent:{agent_type}")

        await self._save_entry(entry, self.episodic_dir)

        logger.info(f"ä¿å­˜æƒ…èŠ‚è®°å¿†: {memory_id}")
        return memory_id

    async def store_memory(
        self,
        content: str,
        memory_type: str = "episodic",
        metadata: Optional[Dict] = None
    ) -> str:
        """å­˜å‚¨è®°å¿† (é€šç”¨åŒ…è£…å™¨)"""
        if memory_type == "episodic":
            return await self.save_episodic_memory(event=content, metadata=metadata)
        elif memory_type == "semantic":
            category = metadata.get("category", "general") if metadata else "general"
            return await self.save_semantic_memory(
                knowledge=content,
                category=category,
                metadata=metadata
            )
        elif memory_type == "procedural":
            category = metadata.get("category", "general") if metadata else "general"
            return await self.save_procedural_memory(
                practice=content,
                category=category,
                metadata=metadata
            )
        else:
            raise MemorySystemError(f"ä¸æ”¯æŒçš„è®°å¿†ç±»åž‹: {memory_type}")

    async def get_episodic_memories(
        self,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """èŽ·å–æœ€è¿‘çš„æƒ…èŠ‚è®°å¿†"""
        MetricsManager.record_memory_op("episodic", "query", "start")
        memories = []
        recent_ids = self.index.get("episodic", [])[-limit:]

        self._clean_expired_cache()

        for memory_id in reversed(recent_ids):
            # å°è¯•ä»Žç¼“å­˜èŽ·å–
            cached_entry = self._get_from_cache("episodic", memory_id)
            if cached_entry:
                memories.append(cached_entry)
                continue

            file_path = self.episodic_dir / f"{memory_id}.json"
            if file_path.exists():
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    entry = json.loads(content)
                    memories.append(entry)
                    # ä¿å­˜åˆ°ç¼“å­˜
                    self._save_to_cache("episodic", memory_id, entry)

        return memories

    # ========== Semantic Memory (è¯­ä¹‰è®°å¿†) ==========

    async def save_semantic_memory(
        self,
        knowledge: str,
        category: str,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """ä¿å­˜è¯­ä¹‰è®°å¿† - è®°å½•é¡¹ç›®çŸ¥è¯†å’Œæž¶æž„å†³ç­–"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        memory_id = self._generate_id(f"semantic_{category}")

        # åŸºç¡€å…ƒæ•°æ®
        base_metadata = {"category": category}
        if metadata:
            base_metadata.update(metadata)

        entry = MemoryEntry(
            memory_id=memory_id,
            memory_type="semantic",
            timestamp=timestamp,
            content=knowledge,
            metadata=base_metadata,
            tags=["semantic", category] + (tags or [])
        )

        await self._save_entry(entry, self.semantic_dir)

        # æ›´æ–°CONTINUITY.md
        await self._append_to_continuity("semantic", knowledge, category)

        logger.info(f"ä¿å­˜è¯­ä¹‰è®°å¿†: {memory_id} (åˆ†ç±»: {category})")
        return memory_id

    async def query_semantic_memory(
        self,
        category: Optional[str] = None,
        keywords: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢è¯­ä¹‰è®°å¿† (åˆ©ç”¨ç´¢å¼•ä¼˜åŒ–ç‰ˆ)"""
        MetricsManager.record_memory_op("semantic", "query", "start")
        memories = []

        # èŽ·å–å¾…æŸ¥ memory_ids
        if category:
            # å¦‚æžœæŒ‡å®šäº†ç±»åˆ«ï¼Œç›´æŽ¥ä»Žç±»åˆ«ç´¢å¼•èŽ·å– ID
            target_ids = self._category_index["semantic"].get(category, [])
        else:
            # å¦åˆ™èŽ·å–æ‰€æœ‰è¯­ä¹‰è®°å¿† ID
            target_ids = self.index.get("semantic", [])

        self._clean_expired_cache()

        # æ‰¹é‡åŠ è½½å’Œè¿‡æ»¤
        for memory_id in target_ids:
            # å°è¯•ä»Žç¼“å­˜èŽ·å–
            entry = self._get_from_cache("semantic", memory_id)

            if not entry:
                file_path = self.semantic_dir / f"{memory_id}.json"
                if not file_path.exists():
                    continue

                try:
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                        entry = json.loads(content)
                        # ä¿å­˜åˆ°ç¼“å­˜
                        self._save_to_cache("semantic", memory_id, entry)
                except (FileNotFoundError, json.JSONDecodeError, UnicodeDecodeError) as e:
                    logger.error(f"åŠ è½½è®°å¿†æ–‡ä»¶å¤±è´¥ (æ–‡ä»¶æŸåæˆ–ç¼–ç é”™è¯¯) {memory_id}: {e}")
                    continue
                except (OSError, IOError) as e:
                    logger.error(f"åŠ è½½è®°å¿†æ–‡ä»¶å¤±è´¥ (ç³»ç»ŸIOé”™è¯¯) {memory_id}: {e}")
                    continue
                except Exception as e:
                    logger.error(f"åŠ è½½è®°å¿†æ–‡ä»¶é‡åˆ°éžé¢„æœŸé”™è¯¯ ({type(e).__name__}) {memory_id}: {e}")
                    continue

            # å…³é”®è¯è¿‡æ»¤
            if keywords:
                content_text = entry.get("content", "").lower()
                if not any(kw.lower() in content_text for kw in keywords):
                    continue

            memories.append(entry)

        return memories

    # ========== Procedural Memory (ç¨‹åºè®°å¿†) ==========

    async def save_procedural_memory(
        self,
        practice: str,
        category: str,
        agent_type: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """ä¿å­˜ç¨‹åºè®°å¿† - å­˜å‚¨æœ€ä½³å®žè·µå’Œå·¥ä½œæµç¨‹"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        memory_id = self._generate_id(f"procedural_{category}")

        # åŸºç¡€å…ƒæ•°æ®
        base_metadata = {"category": category}
        if agent_type:
            base_metadata["agent_type"] = agent_type
        if metadata:
            base_metadata.update(metadata)

        entry = MemoryEntry(
            memory_id=memory_id,
            memory_type="procedural",
            timestamp=timestamp,
            content=practice,
            metadata=base_metadata,
            tags=["procedural", category]
        )

        if agent_type:
            entry.tags.append(f"agent:{agent_type}")

        await self._save_entry(entry, self.procedural_dir)

        # æ›´æ–°CONTINUITY.md
        await self._append_to_continuity("procedural", practice, category)

        logger.info(f"ä¿å­˜ç¨‹åºè®°å¿†: {memory_id} (åˆ†ç±»: {category})")
        return memory_id

    async def _append_to_continuity(
        self,
        memory_type: str,
        content: str,
        category: str
    ) -> None:
        """å¼‚æ­¥æ›´æ–° CONTINUITY.md (ä¼˜åŒ–ç‰ˆï¼šIO ä¸Žå†…å­˜æ“ä½œåˆ†ç¦»)"""
        try:
            # 1. è¯»å–å½“å‰å†…å®¹ (IO å  IO é”)
            async with self._io_lock:
                if self._continuity_cache is None:
                    if self.continuity_file.exists():
                        async with aiofiles.open(self.continuity_file, 'r', encoding='utf-8') as f:
                            full_content = await f.read()
                    else:
                        full_content = "# SuperAgent v3.2 - æŒç»­è®°å¿† (CONTINUITY)\n\n"
                else:
                    full_content = self._continuity_cache

            # 2. åœ¨å†…å­˜ä¸­å¤„ç†å†…å®¹ (ä¸å é”ï¼Œå› ä¸ºæ˜¯å±€éƒ¨å˜é‡)
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            new_entry = f"\n- [{timestamp}] **{category}**: {content}\n"

            if category == "mistake" or "mistake" in content.lower():
                section = SECTION_MISTAKES
            elif memory_type == "procedural":
                section = SECTION_PRACTICES
            elif category == "architecture":
                section = SECTION_ARCHITECTURE
            else:
                section = SECTION_MISTAKES

            if section and section in full_content:
                parts = full_content.split(section)
                full_content = parts[0] + section + new_entry + parts[1]

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            async with self._lock:
                stats = {
                    "total": self.index.get("total_count", 0),
                    "episodic": len(self.index.get("episodic", [])),
                    "semantic": len(self.index.get("semantic", [])),
                    "procedural": len(self.index.get("procedural", [])),
                }

            # 3. æ›´æ–°ç»Ÿè®¡éƒ¨åˆ†å†…å®¹ (å­—ç¬¦ä¸²æ“ä½œ)
            stat_content = f"""## ðŸ“Š é¡¹ç›®ç»Ÿè®¡

- **æ€»è®°å¿†æ¡ç›®**: {stats['total']}
- **æƒ…èŠ‚è®°å¿†**: {stats['episodic']}
- **è¯­ä¹‰è®°å¿†**: {stats['semantic']}
- **ç¨‹åºè®°å¿†**: {stats['procedural']}
- **æœ€åŽæ›´æ–°**: {timestamp}
"""
            if SECTION_STATISTICS in full_content:
                full_content = full_content.split(SECTION_STATISTICS)[0] + stat_content

            # 4. å†™å…¥æ–‡ä»¶ (IO å  IO é”)
            async with self._io_lock:
                async with aiofiles.open(self.continuity_file, 'w', encoding='utf-8') as f:
                    await f.write(full_content)
                self._continuity_cache = full_content

        except (OSError, IOError) as e:
            logger.error(f"æ›´æ–° CONTINUITY.md å¤±è´¥ (IOé”™è¯¯): {e}")
        except Exception as e:
            logger.error(f"æ›´æ–° CONTINUITY.md å¤±è´¥ (æœªçŸ¥é”™è¯¯ - {type(e).__name__}): {e}")

    # ========== ç»¼åˆæŸ¥è¯¢ ==========

    async def query_relevant_memory(
        self,
        task: str,
        agent_type: Optional[str] = None
    ) -> Dict[str, List[str]]:
        """æŸ¥è¯¢ç›¸å…³è®°å¿†,é¿å…é‡å¤é”™è¯¯"""
        relevant = {
            "mistakes": [],
            "best_practices": [],
            "architecture_decisions": []
        }

        try:
            async with aiofiles.open(self.continuity_file, 'r', encoding='utf-8') as f:
                continuity = await f.read()

            if "## ðŸ“ é”™è¯¯ä¸Žæ•™è®­" in continuity:
                relevant["mistakes"].append("æŸ¥çœ‹ .superagent/memory/CONTINUITY.md")

            if "## ðŸŽ¯ æœ€ä½³å®žè·µ" in continuity:
                relevant["best_practices"].append("æŸ¥çœ‹ .superagent/memory/CONTINUITY.md")

            if "## ðŸ—ï¸ æž¶æž„å†³ç­–" in continuity:
                relevant["architecture_decisions"].append("æŸ¥çœ‹ .superagent/memory/CONTINUITY.md")

        except (FileNotFoundError, UnicodeDecodeError) as e:
            logger.error(f"æŸ¥è¯¢ç›¸å…³è®°å¿†å¤±è´¥ (æ–‡ä»¶ç¼ºå¤±æˆ–ç¼–ç é”™è¯¯): {e}")
        except (OSError, IOError) as e:
            logger.error(f"æŸ¥è¯¢ç›¸å…³è®°å¿†å¤±è´¥ (ç³»ç»ŸIOé”™è¯¯): {e}")
        except Exception as e:
            logger.error(f"æŸ¥è¯¢ç›¸å…³è®°å¿†é‡åˆ°éžé¢„æœŸé”™è¯¯ ({type(e).__name__}): {e}")

        return relevant

    async def save_mistake(
        self,
        error: Exception,
        context: str,
        fix: str,
        learning: str
    ) -> None:
        """ä¿å­˜é”™è¯¯æ•™è®­ (é‡æž„ç‰ˆï¼šå§”æ‰˜å¤„ç†)"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        error_name = error.__class__.__name__

        # 1. ä¿å­˜åˆ°æƒ…èŠ‚è®°å¿†
        mistake_entry = f"""## é”™è¯¯: {error_name}
**æ—¶é—´**: {timestamp}
**ä¸Šä¸‹æ–‡**: {context}
**é”™è¯¯ä¿¡æ¯**:
```
{str(error)}
```
**ä¿®å¤æ–¹æ¡ˆ**: {fix}
**ç»éªŒæ•™è®­**: {learning}
---
"""
        await self.save_episodic_memory(
            event=mistake_entry,
            metadata={"type": "mistake", "error_type": error_name}
        )

        # 2. å§”æ‰˜æ›´æ–° CONTINUITY.md
        summary = f"é”™è¯¯: {error_name} | ä¸Šä¸‹æ–‡: {context} | æ–¹æ¡ˆ: {fix}"
        await self._append_to_continuity("episodic", summary, "mistake")

        logger.info(f"ä¿å­˜é”™è¯¯æ•™è®­: {error_name}")

    # ========== ç»Ÿè®¡ä¿¡æ¯ ==========

    def get_statistics(self) -> Dict[str, Any]:
        """èŽ·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯ (v3.3 ä¼˜åŒ–ï¼šåŒ…å«ç¼“å­˜å‘½ä¸­çŽ‡)"""
        idx = self.index

        # v3.3 è®¡ç®—ç¼“å­˜å‘½ä¸­çŽ‡
        total_access = sum(
            sum(entry[1] for entry in cache.values()) + len(cache)
            for cache in self._cache.values()
        )
        cache_hits = sum(
            sum(entry[1] for entry in cache.values())
            for cache in self._cache.values()
        )
        cache_hit_rate = (cache_hits / total_access * 100) if total_access > 0 else 0

        return {
            "total_memories": idx.get("total_count", 0),
            "episodic_count": len(idx.get("episodic", [])),
            "semantic_count": len(idx.get("semantic", [])),
            "procedural_count": len(idx.get("procedural", [])),
            "memory_dir": str(self.memory_dir),
            "cache_size": {t: len(c) for t, c in self._cache.items()},
            "cache_hit_rate": round(cache_hit_rate, 2),
            "index_ready": not self._index_building
        }

    def clear_cache(self) -> None:
        """æ¸…é™¤æ‰€æœ‰æŸ¥è¯¢ç¼“å­˜ (v3.3)"""
        for cache_type in self._cache:
            self._cache[cache_type].clear()
        logger.info("è®°å¿†æŸ¥è¯¢ç¼“å­˜å·²æ¸…é™¤")
