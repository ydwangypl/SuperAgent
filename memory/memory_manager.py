#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è®°å¿†ç®¡ç†å™¨

å®žçŽ°3å±‚è®°å¿†ç³»ç»Ÿ,é˜²æ­¢é‡å¤é”™è¯¯,åŠ é€Ÿé¡¹ç›®å¼€å‘
"""

import asyncio
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
import json
import uuid
import aiofiles
from common.monitoring import MetricsManager
from common.exceptions import MemorySystemError

logger = logging.getLogger(__name__)

# å¸¸é‡å®šä¹‰
SECTION_MISTAKES = "## ðŸ“ é”™è¯¯ä¸Žæ•™è®­"
SECTION_PRACTICES = "## ðŸŽ¯ æœ€ä½³å®žè·µ"
SECTION_ARCHITECTURE = "## ðŸ—ï¸ æž¶æž„å†³ç­–"
SECTION_STATISTICS = "## ðŸ“Š é¡¹ç›®ç»Ÿè®¡"


@dataclass
class MemoryEntry:
    """è®°å¿†æ¡ç›®"""
    memory_id: str
    memory_type: str  # episodic, semantic, procedural
    timestamp: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    tags: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "memory_id": self.memory_id,
            "memory_type": self.memory_type,
            "timestamp": self.timestamp,
            "content": self.content,
            "metadata": self.metadata,
            "tags": self.tags
        }


class MemoryManager:
    """è®°å¿†ç®¡ç†å™¨ - å®žçŽ°3å±‚è®°å¿†ç³»ç»Ÿ (å•ä¾‹å¼‚æ­¥ç‰ˆ)"""

    _instance: Optional['MemoryManager'] = None
    _lock = asyncio.Lock()

    def __new__(cls, *args, **kwargs) -> 'MemoryManager':
        if not cls._instance:
            cls._instance = super(MemoryManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, project_root: Optional[Path] = None) -> None:
        """åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨

        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•
        """
        # ç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡
        if hasattr(self, 'initialized') and self.initialized:
            return

        self.project_root = project_root or Path.cwd()

        # é…ç½®å„å±‚è®°å¿†çš„å­˜å‚¨è·¯å¾„
        self.memory_dir = self.project_root / ".superagent" / "memory"
        self.episodic_dir = self.memory_dir / "episodic"
        self.semantic_dir = self.memory_dir / "semantic"
        self.procedural_dir = self.memory_dir / "procedural"

        # æŒç»­è®°å¿†æ–‡ä»¶
        self.continuity_file = self.memory_dir / "CONTINUITY.md"

        # ç´¢å¼•æ–‡ä»¶
        self.index_file = self.memory_dir / "memory_index.json"

        # é”
        self._lock = asyncio.Lock()  # å†…å­˜çŠ¶æ€é”
        self._io_lock = asyncio.Lock()  # æ–‡ä»¶å†™å…¥é” (é˜²æ­¢å¹¶å‘å†™å…¥åŒä¸€æ–‡ä»¶)

        # åˆ›å»ºç›®å½•ç»“æž„
        self._init_directories()

        # åŠ è½½ç´¢å¼•
        self.index: Dict[str, Any] = self._load_index_sync()

        # åˆå§‹åŒ–æŸ¥è¯¢ç¼“å­˜ (memory_id -> (entry_dict, timestamp))
        self._cache: Dict[str, Dict[str, tuple[Dict[str, Any], float]]] = {
            "episodic": {},
            "semantic": {},
            "procedural": {}
        }
        # ç±»åˆ«ç´¢å¼• (type -> category -> list of memory_ids)
        self._category_index: Dict[str, Dict[str, List[str]]] = {
            "semantic": {},
            "procedural": {}
        }
        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
        self._max_cache_size = 1000  # æ¯ä¸ªç±»åž‹çš„æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        self._continuity_cache: Optional[str] = None  # CONTINUITY.md å†…å®¹ç¼“å­˜
        self._last_flush_time: float = 0.0

        self.initialized = True
        logger.info(f"è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ: {self.memory_dir}")

        # å¼‚æ­¥æž„å»ºç±»åˆ«ç´¢å¼• (ä¸é˜»å¡žåˆå§‹åŒ–)
        asyncio.create_task(self._build_category_index())

    async def _build_category_index(self) -> None:
        """å¼‚æ­¥æž„å»ºç±»åˆ«ç´¢å¼• (ä¼˜åŒ–ç‰ˆï¼šIOä¸åœ¨é”å†…)"""
        for mtype in ["semantic", "procedural"]:
            # 1. å…ˆåœ¨ä¸å é”çš„æƒ…å†µä¸‹æ”¶é›†æ‰€æœ‰ ID
            async with self._lock:
                mids = list(self.index.get(mtype, []))

            for mid in mids:
                try:
                    folder = (self.semantic_dir if mtype == "semantic"
                              else self.procedural_dir)
                    file_path = folder / f"{mid}.json"

                    if file_path.exists():
                        # 2. è¯»å–æ–‡ä»¶ (IO)
                        async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                            content = await f.read()
                            memory_data = json.loads(content)
                            category = memory_data.get("metadata", {}).get(
                                "category", "general"
                            )

                            # 3. æ›´æ–°å†…å­˜ä¸­çš„ç´¢å¼• (å é”)
                            async with self._lock:
                                if category not in self._category_index[mtype]:
                                    self._category_index[mtype][category] = []
                                if mid not in self._category_index[mtype][category]:
                                    self._category_index[mtype][category].append(mid)
                except (FileNotFoundError, json.JSONDecodeError, PermissionError) as e:
                    logger.error(f"æž„å»ºç±»åˆ«ç´¢å¼•å¤±è´¥ - æ–‡ä»¶æŸåæˆ–ä¸¢å¤± ({mid}): {e}")
                except (OSError, IOError) as e:
                    logger.error(f"æž„å»ºç±»åˆ«ç´¢å¼•å¤±è´¥ - IOé”™è¯¯ ({mid}): {e}")
                except Exception as e:
                    logger.error(f"æž„å»ºç±»åˆ«ç´¢å¼•å¤±è´¥ - æœªçŸ¥é”™è¯¯ ({type(e).__name__}) ({mid}): {e}")
        logger.info("ç±»åˆ«ç´¢å¼•æž„å»ºå®Œæˆ")

    def _init_directories(self) -> None:
        """åˆå§‹åŒ–ç›®å½•ç»“æž„"""
        for d in [self.memory_dir, self.episodic_dir, self.semantic_dir, self.procedural_dir]:
            d.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–CONTINUITY.md(å¦‚æžœä¸å­˜åœ¨)
        if not self.continuity_file.exists():
            self._init_continuity_file_sync()

    def _generate_id(self, prefix: str) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„è®°å¿†ID"""
        return f"{prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"

    def _init_continuity_file_sync(self) -> None:
        content = f"""# SuperAgent v3.2 - æŒç»­è®°å¿† (CONTINUITY)

> æ­¤æ–‡ä»¶ç”±SuperAgentè‡ªåŠ¨ç»´æŠ¤,è®°å½•é¡¹ç›®å¼€å‘è¿‡ç¨‹ä¸­çš„é‡è¦ç»éªŒã€é”™è¯¯æ•™è®­å’Œæœ€ä½³å®žè·µ

---

## ðŸ“ é”™è¯¯ä¸Žæ•™è®­ (Mistakes & Learnings)

---

## ðŸŽ¯ æœ€ä½³å®žè·µ (Best Practices)

---

## ðŸ—ï¸ æž¶æž„å†³ç­– (Architecture Decisions)

---

## ðŸ“Š é¡¹ç›®ç»Ÿè®¡ (Project Statistics)

- **æ€»è®°å¿†æ¡ç›®**: 0
- **æƒ…èŠ‚è®°å¿†**: 0
- **è¯­ä¹‰è®°å¿†**: 0
- **ç¨‹åºè®°å¿†**: 0
- **æœ€åŽæ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        self.continuity_file.write_text(content, encoding='utf-8')

    def _load_index_sync(self) -> Dict[str, Any]:
        """åŒæ­¥åŠ è½½è®°å¿†ç´¢å¼•, å¢žåŠ å®¹é”™é€»è¾‘"""
        default_index = {
            "episodic": [],
            "semantic": [],
            "procedural": [],
            "total_count": 0
        }

        if not self.index_file.exists():
            return default_index

        try:
            with open(self.index_file, 'r', encoding='utf-8') as f:
                content = f.read()
                if not content.strip():
                    return default_index
                return json.loads(content)
        except (json.JSONDecodeError, IOError) as e:
            logger.error(f"åŠ è½½è®°å¿†ç´¢å¼•å¤±è´¥, æ–‡ä»¶å¯èƒ½å·²æŸå: {e}ã€‚æ­£åœ¨åˆ›å»ºæ–°ç´¢å¼•ã€‚")
            # å¤‡ä»½æŸåçš„æ–‡ä»¶
            try:
                import shutil
                backup_path = self.index_file.with_suffix('.bak')
                shutil.copy2(self.index_file, backup_path)
                logger.info(f"å·²å°†æŸåçš„ç´¢å¼•å¤‡ä»½è‡³: {backup_path}")
            except (shutil.Error, OSError) as e:
                logger.warning(f"å¤‡ä»½æŸåçš„ç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")
            return default_index

    def _get_from_cache(self, memory_type: str, memory_id: str) -> Optional[Dict[str, Any]]:
        """ä»Žç¼“å­˜èŽ·å–è®°å¿†æ¡ç›®"""
        if memory_type not in self._cache:
            return None

        cached_entry_data = self._cache[memory_type].get(memory_id)
        if not cached_entry_data:
            return None

        entry, timestamp = cached_entry_data
        if time.time() - timestamp > self._cache_ttl:
            del self._cache[memory_type][memory_id]
            return None

        return entry

    def _save_to_cache(self, memory_type: str, memory_id: str, entry: Dict[str, Any]) -> None:
        """ä¿å­˜è®°å¿†æ¡ç›®åˆ°ç¼“å­˜ (å¢žåŠ å®¹é‡é™åˆ¶)"""
        if memory_type in self._cache:
            # å¦‚æžœè¾¾åˆ°æœ€å¤§å®¹é‡ï¼Œåˆ é™¤æœ€æ—©çš„ä¸€ä¸ª (ç®€å•æ·˜æ±°ç­–ç•¥)
            if len(self._cache[memory_type]) >= self._max_cache_size:
                # æ‰¾åˆ°æœ€æ—©çš„æ—¶é—´æˆ³
                oldest_id = min(
                    self._cache[memory_type].keys(),
                    key=lambda k: self._cache[memory_type][k][1]
                )
                del self._cache[memory_type][oldest_id]

            self._cache[memory_type][memory_id] = (entry, time.time())

    def _clean_expired_cache(self) -> None:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        now = time.time()
        for memory_type in self._cache:
            expired_keys = [
                k for k, (v, t) in self._cache[memory_type].items()
                if now - t > self._cache_ttl
            ]
            for k in expired_keys:
                del self._cache[memory_type][k]

    async def _save_index(self) -> None:
        """å¼‚æ­¥ä¿å­˜è®°å¿†ç´¢å¼• (å¸¦ IO é”ä¿æŠ¤)"""
        try:
            # 1. å…ˆå‡†å¤‡è¦å†™å…¥çš„å†…å®¹ (å†…å­˜æ“ä½œ)
            async with self._lock:
                content = json.dumps(self.index, indent=2, ensure_ascii=False)

            # 2. èŽ·å– IO é”å¹¶å†™å…¥æ–‡ä»¶
            async with self._io_lock:
                # ä½¿ç”¨åŽŸå­å†™å…¥ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶å†é‡å‘½å
                temp_file = self.index_file.with_suffix('.tmp')
                async with aiofiles.open(temp_file, 'w', encoding='utf-8') as f:
                    await f.write(content)

                # Windows ä¸‹ replace å‰å¦‚æžœç›®æ ‡æ–‡ä»¶å­˜åœ¨éœ€è¦å¤„ç†ï¼Œä½† Path.replace åº”è¯¥å¯ä»¥å¤„ç†
                temp_file.replace(self.index_file)

        except (OSError, IOError) as e:
            logger.error(f"ä¿å­˜è®°å¿†ç´¢å¼•å¤±è´¥ (IOé”™è¯¯): {e}")
            raise MemorySystemError(f"ä¿å­˜è®°å¿†ç´¢å¼•å¤±è´¥ (IOé”™è¯¯): {str(e)}")
        except (TypeError, ValueError) as e:
            logger.error(f"ä¿å­˜è®°å¿†ç´¢å¼•å¤±è´¥ (åºåˆ—åŒ–é”™è¯¯): {e}")
            raise MemorySystemError(f"ä¿å­˜è®°å¿†ç´¢å¼•å¤±è´¥ (åºåˆ—åŒ–é”™è¯¯): {str(e)}")
        except Exception as e:
            logger.error(f"ä¿å­˜è®°å¿†ç´¢å¼•é‡åˆ°æœªçŸ¥é”™è¯¯ ({type(e).__name__}): {e}")
            raise MemorySystemError(
                f"ä¿å­˜è®°å¿†ç´¢å¼•é‡åˆ°æœªçŸ¥é”™è¯¯ ({type(e).__name__}): {str(e)}"
            )

    async def _save_entry(self, entry: MemoryEntry, directory: Path) -> None:
        """é€šç”¨æ¡ç›®ä¿å­˜æ–¹æ³• (å·²ä¼˜åŒ–ï¼šå‰¥ç¦» IO é”)"""
        try:
            file_path = directory / f"{entry.memory_id}.json"
            entry_dict = entry.to_dict()

            # 1. å…ˆå†™æ¡ç›®æ–‡ä»¶ (ä¸å é”)
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(entry_dict, indent=2, ensure_ascii=False))

            # 2. æ›´æ–°ç¼“å­˜å’Œå†…å­˜ç´¢å¼• (å é”)
            async with self._lock:
                # ä¿å­˜åˆ°ç¼“å­˜
                self._save_to_cache(entry.memory_type, entry.memory_id, entry_dict)

                # æ›´æ–°ç´¢å¼•åˆ—è¡¨
                if entry.memory_id not in self.index[entry.memory_type]:
                    self.index[entry.memory_type].append(entry.memory_id)
                    self.index["total_count"] += 1

                # æ›´æ–°ç±»åˆ«ç´¢å¼•
                if entry.memory_type in self._category_index:
                    category = entry.metadata.get("category", "general")
                    if category not in self._category_index[entry.memory_type]:
                        self._category_index[entry.memory_type][category] = []
                    if entry.memory_id not in \
                            self._category_index[entry.memory_type][category]:
                        self._category_index[entry.memory_type][category].append(
                            entry.memory_id
                        )

            # 3. å¼‚æ­¥ä¿å­˜ç´¢å¼•æ–‡ä»¶ (IO å¯†é›†ï¼Œç§»å‡ºä¸»é”)
            await self._save_index()

            # 4. æ›´æ–°ç›‘æŽ§æŒ‡æ ‡
            MetricsManager.record_memory_op(entry.memory_type, "save", "success")
            # æ­¤æ—¶è®¿é—® self.index éœ€è¦æ³¨æ„å¹¶å‘ï¼Œä½†åœ¨ record è¿™ç§éžå…³é”®æ“ä½œä¸­é€šå¸¸ OK
            MetricsManager.update_memory_size(
                entry.memory_type,
                len(self.index[entry.memory_type])
            )

        except (OSError, IOError, PermissionError) as e:
            logger.error(f"ä¿å­˜è®°å¿†æ¡ç›®å¤±è´¥ (æ–‡ä»¶æˆ–ç£ç›˜é”™è¯¯): {e}")
            MetricsManager.record_memory_op(entry.memory_type, "save", "error")
            raise MemorySystemError(f"ä¿å­˜è®°å¿†æ¡ç›®å¤±è´¥ (IOé”™è¯¯): {str(e)}")
        except MemorySystemError:
            MetricsManager.record_memory_op(entry.memory_type, "save", "error")
            raise
        except Exception as e:
            logger.error(f"ä¿å­˜è®°å¿†æ¡ç›®å¤±è´¥ (æœªçŸ¥é”™è¯¯ - {type(e).__name__}): {e}")
            MetricsManager.record_memory_op(entry.memory_type, "save", "error")
            raise MemorySystemError(
                f"ä¿å­˜è®°å¿†æ¡ç›®é‡åˆ°æœªçŸ¥é”™è¯¯ ({type(e).__name__}): {str(e)}"
            )

    # ========== Episodic Memory (æƒ…èŠ‚è®°å¿†) ==========

    async def save_episodic_memory(
        self,
        event: str,
        task_id: Optional[str] = None,
        agent_type: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """ä¿å­˜æƒ…èŠ‚è®°å¿† - è®°å½•ä»»åŠ¡æ‰§è¡ŒåŽ†å²"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        memory_id = self._generate_id("episodic")

        entry = MemoryEntry(
            memory_id=memory_id,
            memory_type="episodic",
            timestamp=timestamp,
            content=event,
            metadata=metadata or {},
            tags=["episodic"]
        )

        if task_id:
            entry.tags.append(f"task:{task_id}")
        if agent_type:
            entry.tags.append(f"agent:{agent_type}")

        await self._save_entry(entry, self.episodic_dir)

        logger.info(f"ä¿å­˜æƒ…èŠ‚è®°å¿†: {memory_id}")
        return memory_id

    async def store_memory(
        self,
        content: str,
        memory_type: str = "episodic",
        metadata: Optional[Dict] = None
    ) -> str:
        """å­˜å‚¨è®°å¿† (é€šç”¨åŒ…è£…å™¨)"""
        if memory_type == "episodic":
            return await self.save_episodic_memory(event=content, metadata=metadata)
        elif memory_type == "semantic":
            category = metadata.get("category", "general") if metadata else "general"
            return await self.save_semantic_memory(
                knowledge=content,
                category=category,
                metadata=metadata
            )
        elif memory_type == "procedural":
            category = metadata.get("category", "general") if metadata else "general"
            return await self.save_procedural_memory(
                practice=content,
                category=category,
                metadata=metadata
            )
        else:
            raise MemorySystemError(f"ä¸æ”¯æŒçš„è®°å¿†ç±»åž‹: {memory_type}")

    async def get_episodic_memories(
        self,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """èŽ·å–æœ€è¿‘çš„æƒ…èŠ‚è®°å¿†"""
        MetricsManager.record_memory_op("episodic", "query", "start")
        memories = []
        recent_ids = self.index.get("episodic", [])[-limit:]

        self._clean_expired_cache()

        for memory_id in reversed(recent_ids):
            # å°è¯•ä»Žç¼“å­˜èŽ·å–
            cached_entry = self._get_from_cache("episodic", memory_id)
            if cached_entry:
                memories.append(cached_entry)
                continue

            file_path = self.episodic_dir / f"{memory_id}.json"
            if file_path.exists():
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    entry = json.loads(content)
                    memories.append(entry)
                    # ä¿å­˜åˆ°ç¼“å­˜
                    self._save_to_cache("episodic", memory_id, entry)

        return memories

    # ========== Semantic Memory (è¯­ä¹‰è®°å¿†) ==========

    async def save_semantic_memory(
        self,
        knowledge: str,
        category: str,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """ä¿å­˜è¯­ä¹‰è®°å¿† - è®°å½•é¡¹ç›®çŸ¥è¯†å’Œæž¶æž„å†³ç­–"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        memory_id = self._generate_id(f"semantic_{category}")

        # åŸºç¡€å…ƒæ•°æ®
        base_metadata = {"category": category}
        if metadata:
            base_metadata.update(metadata)

        entry = MemoryEntry(
            memory_id=memory_id,
            memory_type="semantic",
            timestamp=timestamp,
            content=knowledge,
            metadata=base_metadata,
            tags=["semantic", category] + (tags or [])
        )

        await self._save_entry(entry, self.semantic_dir)

        # æ›´æ–°CONTINUITY.md
        await self._append_to_continuity("semantic", knowledge, category)

        logger.info(f"ä¿å­˜è¯­ä¹‰è®°å¿†: {memory_id} (åˆ†ç±»: {category})")
        return memory_id

    async def query_semantic_memory(
        self,
        category: Optional[str] = None,
        keywords: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢è¯­ä¹‰è®°å¿† (åˆ©ç”¨ç´¢å¼•ä¼˜åŒ–ç‰ˆ)"""
        MetricsManager.record_memory_op("semantic", "query", "start")
        memories = []

        # èŽ·å–å¾…æŸ¥ memory_ids
        if category:
            # å¦‚æžœæŒ‡å®šäº†ç±»åˆ«ï¼Œç›´æŽ¥ä»Žç±»åˆ«ç´¢å¼•èŽ·å– ID
            target_ids = self._category_index["semantic"].get(category, [])
        else:
            # å¦åˆ™èŽ·å–æ‰€æœ‰è¯­ä¹‰è®°å¿† ID
            target_ids = self.index.get("semantic", [])

        self._clean_expired_cache()

        # æ‰¹é‡åŠ è½½å’Œè¿‡æ»¤
        for memory_id in target_ids:
            # å°è¯•ä»Žç¼“å­˜èŽ·å–
            entry = self._get_from_cache("semantic", memory_id)

            if not entry:
                file_path = self.semantic_dir / f"{memory_id}.json"
                if not file_path.exists():
                    continue

                try:
                    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                        content = await f.read()
                        entry = json.loads(content)
                        # ä¿å­˜åˆ°ç¼“å­˜
                        self._save_to_cache("semantic", memory_id, entry)
                except (FileNotFoundError, json.JSONDecodeError, UnicodeDecodeError) as e:
                    logger.error(f"åŠ è½½è®°å¿†æ–‡ä»¶å¤±è´¥ (æ–‡ä»¶æŸåæˆ–ç¼–ç é”™è¯¯) {memory_id}: {e}")
                    continue
                except (OSError, IOError) as e:
                    logger.error(f"åŠ è½½è®°å¿†æ–‡ä»¶å¤±è´¥ (ç³»ç»ŸIOé”™è¯¯) {memory_id}: {e}")
                    continue
                except Exception as e:
                    logger.error(f"åŠ è½½è®°å¿†æ–‡ä»¶é‡åˆ°éžé¢„æœŸé”™è¯¯ ({type(e).__name__}) {memory_id}: {e}")
                    continue

            # å…³é”®è¯è¿‡æ»¤
            if keywords:
                content_text = entry.get("content", "").lower()
                if not any(kw.lower() in content_text for kw in keywords):
                    continue

            memories.append(entry)

        return memories

    # ========== Procedural Memory (ç¨‹åºè®°å¿†) ==========

    async def save_procedural_memory(
        self,
        practice: str,
        category: str,
        agent_type: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """ä¿å­˜ç¨‹åºè®°å¿† - å­˜å‚¨æœ€ä½³å®žè·µå’Œå·¥ä½œæµç¨‹"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        memory_id = self._generate_id(f"procedural_{category}")

        # åŸºç¡€å…ƒæ•°æ®
        base_metadata = {"category": category}
        if agent_type:
            base_metadata["agent_type"] = agent_type
        if metadata:
            base_metadata.update(metadata)

        entry = MemoryEntry(
            memory_id=memory_id,
            memory_type="procedural",
            timestamp=timestamp,
            content=practice,
            metadata=base_metadata,
            tags=["procedural", category]
        )

        if agent_type:
            entry.tags.append(f"agent:{agent_type}")

        await self._save_entry(entry, self.procedural_dir)

        # æ›´æ–°CONTINUITY.md
        await self._append_to_continuity("procedural", practice, category)

        logger.info(f"ä¿å­˜ç¨‹åºè®°å¿†: {memory_id} (åˆ†ç±»: {category})")
        return memory_id

    async def _append_to_continuity(
        self,
        memory_type: str,
        content: str,
        category: str
    ) -> None:
        """å¼‚æ­¥æ›´æ–° CONTINUITY.md (ä¼˜åŒ–ç‰ˆï¼šIO ä¸Žå†…å­˜æ“ä½œåˆ†ç¦»)"""
        try:
            # 1. è¯»å–å½“å‰å†…å®¹ (IO å  IO é”)
            async with self._io_lock:
                if self._continuity_cache is None:
                    if self.continuity_file.exists():
                        async with aiofiles.open(self.continuity_file, 'r', encoding='utf-8') as f:
                            full_content = await f.read()
                    else:
                        full_content = "# SuperAgent v3.2 - æŒç»­è®°å¿† (CONTINUITY)\n\n"
                else:
                    full_content = self._continuity_cache

            # 2. åœ¨å†…å­˜ä¸­å¤„ç†å†…å®¹ (ä¸å é”ï¼Œå› ä¸ºæ˜¯å±€éƒ¨å˜é‡)
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            new_entry = f"\n- [{timestamp}] **{category}**: {content}\n"

            if category == "mistake" or "mistake" in content.lower():
                section = SECTION_MISTAKES
            elif memory_type == "procedural":
                section = SECTION_PRACTICES
            elif category == "architecture":
                section = SECTION_ARCHITECTURE
            else:
                section = SECTION_MISTAKES

            if section and section in full_content:
                parts = full_content.split(section)
                full_content = parts[0] + section + new_entry + parts[1]

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            async with self._lock:
                stats = {
                    "total": self.index.get("total_count", 0),
                    "episodic": len(self.index.get("episodic", [])),
                    "semantic": len(self.index.get("semantic", [])),
                    "procedural": len(self.index.get("procedural", [])),
                }

            # 3. æ›´æ–°ç»Ÿè®¡éƒ¨åˆ†å†…å®¹ (å­—ç¬¦ä¸²æ“ä½œ)
            stat_content = f"""## ðŸ“Š é¡¹ç›®ç»Ÿè®¡

- **æ€»è®°å¿†æ¡ç›®**: {stats['total']}
- **æƒ…èŠ‚è®°å¿†**: {stats['episodic']}
- **è¯­ä¹‰è®°å¿†**: {stats['semantic']}
- **ç¨‹åºè®°å¿†**: {stats['procedural']}
- **æœ€åŽæ›´æ–°**: {timestamp}
"""
            if SECTION_STATISTICS in full_content:
                full_content = full_content.split(SECTION_STATISTICS)[0] + stat_content

            # 4. å†™å…¥æ–‡ä»¶ (IO å  IO é”)
            async with self._io_lock:
                async with aiofiles.open(self.continuity_file, 'w', encoding='utf-8') as f:
                    await f.write(full_content)
                self._continuity_cache = full_content

        except (OSError, IOError) as e:
            logger.error(f"æ›´æ–° CONTINUITY.md å¤±è´¥ (IOé”™è¯¯): {e}")
        except Exception as e:
            logger.error(f"æ›´æ–° CONTINUITY.md å¤±è´¥ (æœªçŸ¥é”™è¯¯ - {type(e).__name__}): {e}")

    # ========== ç»¼åˆæŸ¥è¯¢ ==========

    async def query_relevant_memory(
        self,
        task: str,
        agent_type: Optional[str] = None
    ) -> Dict[str, List[str]]:
        """æŸ¥è¯¢ç›¸å…³è®°å¿†,é¿å…é‡å¤é”™è¯¯"""
        relevant = {
            "mistakes": [],
            "best_practices": [],
            "architecture_decisions": []
        }

        try:
            async with aiofiles.open(self.continuity_file, 'r', encoding='utf-8') as f:
                continuity = await f.read()

            if "## ðŸ“ é”™è¯¯ä¸Žæ•™è®­" in continuity:
                relevant["mistakes"].append("æŸ¥çœ‹ .superagent/memory/CONTINUITY.md")

            if "## ðŸŽ¯ æœ€ä½³å®žè·µ" in continuity:
                relevant["best_practices"].append("æŸ¥çœ‹ .superagent/memory/CONTINUITY.md")

            if "## ðŸ—ï¸ æž¶æž„å†³ç­–" in continuity:
                relevant["architecture_decisions"].append("æŸ¥çœ‹ .superagent/memory/CONTINUITY.md")

        except (FileNotFoundError, UnicodeDecodeError) as e:
            logger.error(f"æŸ¥è¯¢ç›¸å…³è®°å¿†å¤±è´¥ (æ–‡ä»¶ç¼ºå¤±æˆ–ç¼–ç é”™è¯¯): {e}")
        except (OSError, IOError) as e:
            logger.error(f"æŸ¥è¯¢ç›¸å…³è®°å¿†å¤±è´¥ (ç³»ç»ŸIOé”™è¯¯): {e}")
        except Exception as e:
            logger.error(f"æŸ¥è¯¢ç›¸å…³è®°å¿†é‡åˆ°éžé¢„æœŸé”™è¯¯ ({type(e).__name__}): {e}")

        return relevant

    async def save_mistake(
        self,
        error: Exception,
        context: str,
        fix: str,
        learning: str
    ) -> None:
        """ä¿å­˜é”™è¯¯æ•™è®­ (é‡æž„ç‰ˆï¼šå§”æ‰˜å¤„ç†)"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        error_name = error.__class__.__name__

        # 1. ä¿å­˜åˆ°æƒ…èŠ‚è®°å¿†
        mistake_entry = f"""## é”™è¯¯: {error_name}
**æ—¶é—´**: {timestamp}
**ä¸Šä¸‹æ–‡**: {context}
**é”™è¯¯ä¿¡æ¯**:
```
{str(error)}
```
**ä¿®å¤æ–¹æ¡ˆ**: {fix}
**ç»éªŒæ•™è®­**: {learning}
---
"""
        await self.save_episodic_memory(
            event=mistake_entry,
            metadata={"type": "mistake", "error_type": error_name}
        )

        # 2. å§”æ‰˜æ›´æ–° CONTINUITY.md
        summary = f"é”™è¯¯: {error_name} | ä¸Šä¸‹æ–‡: {context} | æ–¹æ¡ˆ: {fix}"
        await self._append_to_continuity("episodic", summary, "mistake")

        logger.info(f"ä¿å­˜é”™è¯¯æ•™è®­: {error_name}")

    # ========== ç»Ÿè®¡ä¿¡æ¯ ==========

    def get_statistics(self) -> Dict[str, Any]:
        """èŽ·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯ (çº¿ç¨‹å®‰å…¨ä¼˜åŒ–)"""
        # æ³¨æ„ï¼šè¿™é‡Œä¸ä½¿ç”¨ await self._lockï¼Œå› ä¸ºå®ƒæ˜¯åŒæ­¥æ–¹æ³•
        # åœ¨ Python ä¸­è¯»å–å­—å…¸é€šå¸¸æ˜¯å®‰å…¨çš„ï¼Œä½†æˆ‘ä»¬å–å¿«ç…§
        idx = self.index
        return {
            "total_memories": idx.get("total_count", 0),
            "episodic_count": len(idx.get("episodic", [])),
            "semantic_count": len(idx.get("semantic", [])),
            "procedural_count": len(idx.get("procedural", [])),
            "memory_dir": str(self.memory_dir),
            "cache_size": {t: len(c) for t, c in self._cache.items()}
        }

    def clear_cache(self) -> None:
        """æ¸…é™¤æ‰€æœ‰æŸ¥è¯¢ç¼“å­˜"""
        for cache_type in self._cache:
            self._cache[cache_type].clear()
        logger.info("è®°å¿†æŸ¥è¯¢ç¼“å­˜å·²æ¸…é™¤")
